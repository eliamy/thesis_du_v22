# Almost same as function for grisearch of svm but adjusted for random forest parameters 

def grid_eval_rf(data, features_to_test, oversampling_bol, anova_bol, pca_bol, param_list_rf):
    best_params = []
    best_f1 = 0

    for k in range(len(param_list_rf)):
            print(k)
            model = RandomForestClassifier(
                        max_samples = param_list_rf[k][0],
                        max_depth = param_list_rf[k][1],
                        max_features = param_list_rf[k][2],
                        min_samples_leaf = param_list_rf[k][3],
                        min_samples_split = param_list_rf[k][4],
                        n_estimators = param_list_rf[k][5],
                        criterion = param_list_rf[k][6], n_jobs = -1)

            predictions, true_label = evaluate_model(data, model, features_to_test, 
                                                     over = oversampling_bol, do_anova = anova_bol, do_pca = pca_bol)

            y_true = numpy.concatenate(true_label)
            y_pred = numpy.concatenate(predictions)

            f_one = f1_score(y_true, y_pred, average = 'macro')

            if f_one > best_f1:
                best_f1 = f_one
                best_pred = y_pred
                best_y = y_true

                best_params = [param_list_rf[k][0], param_list_rf[k][1], param_list_rf[k][2], param_list_rf[k][3],
                               param_list_rf[k][4], param_list_rf[k][5], param_list_rf[k][6]]
    return(best_pred, best_y, best_params)
    
    
#Example of grid search for random forest

# Parameters to be tested in all different combinations.
# starting of using larger step-sizes between parameters then running again with reduced "search-grid" with smaller step sizes
max_samples = [0.3, 0.5, 0.8]
max_depth = [None, 30, 50, 60]
max_features = ['sqrt', 'auto']
min_samples_leaf = [2,5,10]
min_samples_split = [2, 5, 10]
n_estimators = [200, 500, 750]
criterion = ['entropy','gini']

param_list_rf = list(itertools.product(max_samples,max_depth,max_features,min_samples_leaf,
                                       min_samples_split,n_estimators, criterion))

#Features in base format, with oversampling (II)
best_pred_rf, best_y_rf, best_params_rf = grid_eval_rf(data, features, True, True, False, param_list_rf)
print('\n*Classification Report:\n',classification_report(best_y_rf, best_pred_rf, target_names = target_names))
print(best_params_rf)                                       
                                       
