import pandas
import sklearn
import numpy
import scipy
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import f1_score
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
import seaborn as sns
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import SelectKBest
from sklearn.preprocessing import StandardScaler
import itertools


#Function to use anova for feature selection. Function return column index of features with significance < 0.05
def anova_selection(training_x, training_y):
    fs = SelectKBest(score_func=f_classif)
    
    # learn relationship from training data
    fs.fit(training_x, training_y)
    
    # get only those variables with p_value smaller than 0.05
    sign_index = [i for i in range(len(fs.pvalues_)) if fs.pvalues_[i] < 0.05]
    
    return(sign_index)


#Function to use PCA for dimensionality reduction. Return PCA components for training and test set. 
def pca_selection(training_x, testing_x):
    # Standardizing the features
    x_train = sklearn.preprocessing.StandardScaler().fit_transform(training_x)
    x_test = sklearn.preprocessing.StandardScaler().fit_transform(testing_x)

    pca = sklearn.decomposition.PCA(0.80) #different setting has been tried for variance explained
    principal_components_train = pandas.DataFrame(pca.fit_transform(x_train)) #fit to training and transform
    principal_components_test = pandas.DataFrame(pca.transform(x_test)) #tranform also test set
    
    return(principal_components_train, principal_components_test)
    
# Function to facilitate oversampling, pca, anova within cross-validation. Can possibly be handled a lot faster with a pipeline
# Parameters are the dataset(dataframe), classification model, name of feature columns, bool parameters for doing oversampling, pca and anova
def evaluate_model(data_full, classification_model, feature_name, over, do_anova, do_pca):
    kf = StratifiedKFold(n_splits = 5, random_state=1, shuffle = True)
    model = classification_model
    
    #different types of oversampling testet, SMOTE proved better in terms of performance metrics
    #oversample = RandomOverSampler() 
    
    #"best" way of oversampling
    oversample = SMOTE()

    preds = []
    true_labels = []
    

    for train_index, test_index in kf.split(data_full[feature_name], data_full['road class']):
                
        train_x = data_full[feature_name].iloc[train_index] #get features of training folds
        train_y = data_full['road class'].iloc[train_index] # get target of training fold
 
        test_x = data_full[feature_name].iloc[test_index]   #get features of test fold
        test_y = data_full['road class'].iloc[test_index] #get target of test fold
        
        if do_anova == True:
            sign_index = anova_selection(train_x, train_y) #gets indices of significant features
            train_x = train_x.iloc[:, sign_index]
            test_x = test_x.iloc[:, sign_index]
            
            # Scaling only done when using SVM as classifier..
            #scaler = StandardScaler() 
            #train_x[train_x.columns] = scaler.fit_transform(train_x[train_x.columns])
            #test_x[test_x.columns] = scaler.transform(test_x[test_x.columns])
                
        if do_pca == True:
            train_x, test_x = pca_selection(train_x, test_x) #gets train and test as pca components instead
        
        if over == True:
            # oversample only the training data during each fold
            train_x_over, train_y_over = oversample.fit_resample(train_x, train_y) 
            #print(sorted(Counter(train_y_over).items()))
            model = model.fit(train_x_over, train_y_over)
        
        else:
            #print(sorted(Counter(train_y).items()))
            model = model.fit(train_x, train_y)
        
        predictions = model.predict(test_x)
        preds.append(predictions)
        
        true_labels.append(test_y)
        
    return(preds, true_labels)


# Example of how one can use the function above to asses model performance
data = pandas.read_csv('C:/Users/Some_User/Some_Folder/extracted_sound_features.csv', header=0)
#drop all observations marked "removable", should be done prior to feature extraction to save time...
data = data.loc[data['road class'] != 'Removab']
#dropping "Removable"-observations then have to reset the index of the dataframe to match current number of rows
data.reset_index(inplace=True)
# Loading from csv, resetting indeces creates some redundant columns, here they are dropped
data.drop(columns=data.columns[0:2], 
        axis=1, 
        inplace=True)

# Put names of features in a list
column_names = list(data.columns.values)

#getting only the names of the feature columns
features = column_names[1:]

# used for plotting confusion matrices
target_names = ['Class 1', 'Class 2', 'Class 3', 'Class 4']

# Calling the evaluation function
predictions, true_label = evaluate_model(data, SVC(kernel='rbf', gamma = 0.01, C = 1), features, 
                                                     over = True, do_anova = True, do_pca = False)
y_true = numpy.concatenate(true_label)
y_pred = numpy.concatenate(predictions)

print('\n*Classification Report:\n',classification_report(y_true, y_pred, target_names = target_names, digits = 4))
